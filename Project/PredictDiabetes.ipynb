{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9e230ddf-2c78-4214-81e0-dc1a18bb47c1",
   "metadata": {},
   "source": [
    "# Diabetes Prediction\n",
    "#### Data Source\n",
    "From Kaggle by user Mohammed Mustafa \n",
    "Link https://www.kaggle.com/datasets/iammustafatz/diabetes-prediction-dataset\n",
    "### Description\n",
    "The data contains predictors `gender`, `age`, `hypertension`, `heart_disease`, `smoking_history`, `bmi`, `HbA1c_level`, `blood_glucose_level`, and the response `diabetes` for 100000 patients.\n",
    "- `gender` may have one of three qualitative values `female`, `male`, or `other`\n",
    "- `age` may be any integer value from 0-80\n",
    "- `hypertension` is one-hot encoded where 0 and 1 are the absense and presence of hypertension exhibited by the patient, respectively\n",
    "- `heart_disease` is one-hot encoded where 0 and 1 are the absense and presence of heart disease exhibited by the patient, respectively\n",
    "- `smoking_history` may be one of six qualitative values, `not current`, `former`, `No Info`, `current`, `never`, and `ever`\n",
    "- `bmi` is the body mass index of the patient and may be any real number between 10 and 95.7\n",
    "- `HbA1c_level` is the level of hemoglobic A1c measured in the pateints blood and may be a real number from 3.5 to 9\n",
    "- `blood_glucose_level` is the level of glucose in the patient's bloodstream and may be a real nmumber from 80 to 300\n",
    "- `diabetes` is one-hot encoded where 0 and 1 are the absense and presence of diabetes exhibited by the patient, respectively\n",
    "\n",
    "### Training and Test Split\n",
    "The data will be randomly split into training and test data. 10% of the data will be removed exclusively for model testing and analysis toward the end of the experiment. The remaining 90% of data will be used for training purposes. \n",
    "\n",
    "## Centralized and Federated Environments\n",
    "### Centralized\n",
    "In this scenario, all data is stored in one location and training using a single logistic regression model. This model will be analyzed and provide a baseline against which the federated learning tests will be compared.\n",
    "\n",
    "### Federated\n",
    "In this scenario, the training data is randomly distributed into 8 different smaller sets which will be used to train 8 invididual logistic regression models concurrently. This will simulate 8 different clients training on their own local data. Initially, the bins will contain roughly the same amount of data. To explore the effects of an imbalanced distribution of data, another experiment will assess model training when this data is not equally distributed across the clients. It is important to note that between training rounds, the clients will send model parameters (in this case the model coefficients) to a central process acting as a server. This server will aggregate model updates to generate a new global model which all clients will use to update their own local model before the next round of training.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fcdf28d6-8e8c-438e-96a7-7d673b46e450",
   "metadata": {},
   "source": [
    "## Installing Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7ad823b6-d5d0-42eb-93c4-123629347e75",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install numpy pandas sklearn"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02c9f75c-6e44-44f4-8058-aa7758623f42",
   "metadata": {},
   "source": [
    "## Initial Data Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0aa0a99b-e112-4cbe-a939-5a8d68193365",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'C:\\\\Repositories\\\\COMP8590-StatisticalLearning\\\\data'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "# Change directory to the location holding the data\n",
    "os.chdir('../data')\n",
    "os.getcwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "61137dbf-affc-4b59-9511-dc511c30bf4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "from math import ceil\n",
    "import pandas as pd\n",
    "import sys\n",
    "\n",
    "random.seed(0)\n",
    "# Split the data, 90% Training, 10% Test\n",
    "# Test data only used for final evaluation\n",
    "\n",
    "# Count lines in file\n",
    "data_file = open('diabetes_prediction_dataset.csv')\n",
    "lines_data = sum(1 for line in data_file)\n",
    "\n",
    "# Reset file pointer\n",
    "data_file.seek(0)\n",
    "\n",
    "# Randomly Select Train and test Samples\n",
    "select_test = random.sample(range(1, lines_data), ceil(lines_data * 0.1))\n",
    "select_train = [row for row in range(1, lines_data) if row not in select_test]\n",
    "\n",
    "# Load in test data\n",
    "train_df = pd.read_csv('diabetes_prediction_dataset.csv', skiprows=select_test)\n",
    "test_df = pd.read_csv('diabetes_prediction_dataset.csv', skiprows=select_train)\n",
    "\n",
    "# As an additional preprocessing task, we need to create dummy variables for the `smoking_history` predictor.\n",
    "# This is necessary so that we can create 6 dummy dummy variables that will be one-hot encoded for each of the\n",
    "# possible categorical values of `smoking_history`. The new dummy variables are `smoking_history_No Info`,\n",
    "# `smoking_history_current`, `smoking_history_ever`, `smoking_history_former`, `smoking_history_never`, and\n",
    "# `smoking_history_not current`.\n",
    "smk_dummies_train = pd.get_dummies(train_df['smoking_history'], dtype=int)\n",
    "train_df = train_df.drop(columns=['smoking_history'])\n",
    "for column in smk_dummies_train.columns:\n",
    "        train_df.insert(7, 'smoking_history_' + column, smk_dummies_train[column])\n",
    "\n",
    "smk_dummies_test = pd.get_dummies(test_df['smoking_history'], dtype=int)\n",
    "test_df = test_df.drop(columns=['smoking_history'])\n",
    "for column in smk_dummies_test.columns:\n",
    "        test_df.insert(7, 'smoking_history_' + column, smk_dummies_test[column])\n",
    "        \n",
    "# The same process must also be performed for the `gender` predictor. We will create three dummy variables, \n",
    "# `female`, `male`, and `other`, and one-hot encode them in a similar fashion to above\n",
    "gnd_dummies_train = pd.get_dummies(train_df['gender'], dtype=int)\n",
    "train_df = train_df.drop(columns=['gender'])\n",
    "for column in gnd_dummies_train.columns:\n",
    "        train_df.insert(0, 'gender_' + column, gnd_dummies_train[column])\n",
    "\n",
    "gnd_dummies_test = pd.get_dummies(test_df['gender'], dtype=int)\n",
    "test_df = test_df.drop(columns=['gender'])\n",
    "for column in gnd_dummies_test.columns:\n",
    "        test_df.insert(0, 'gender_' + column, gnd_dummies_test[column])\n",
    "\n",
    "# Confirm data added correctly\n",
    "# print(train_df.shape)\n",
    "# print(train_df)\n",
    "# print(test_df.shape)\n",
    "# print(test_df)\n",
    "\n",
    "# For consistency later and to avoid repeating this step, we will save the training and test data to separate files\n",
    "train_df.to_csv('diabetes_prediction_dataset_train.csv', index=False)\n",
    "test_df.to_csv('diabetes_prediction_dataset_test.csv', index=False) # Note, this will be reserved exclusively for model analysis AFTER training"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce8db592-a01d-466b-be6e-8f02ffd19420",
   "metadata": {},
   "source": [
    "# Centralized Logistic Regression\n",
    "## Scenario\n",
    "In this scenario, we create a logistic regression model that is trained to predict the reponse `diabetes` using the predictors `gender`, `age`, `hypertension`, `heart_disease`, `smoking_history`, `bmi`, `HbA1c_level`, and `blood_glucose_level`. We assume that all training data and the training task itself is centralized to one system. Here, we train the model using k-fold cross-validation."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01452610-476e-4fb2-8509-a2867769d4d1",
   "metadata": {},
   "source": [
    "### Implementation\n",
    "#### Data Processing\n",
    "We begin by importing the necessary packages and processing the training data. Here we require the training data to be split again into training and validation data. Here, the traininig set is relatively large (~90000 samples) so we implement k-fold cross-validation where k=10. For this task, scikit-learn fortunately has a built-in function that will train the logistic regession model with cross validation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "dc6e82f7-8ad9-44f1-858d-dfd55c1344e1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size:\n",
      "\tcent_train:\t(89999, 16)\n",
      "\tX_train:\t(89999, 15)\n",
      "\tY_train:\t(89999,)\n"
     ]
    }
   ],
   "source": [
    "# imports \n",
    "from sklearn.linear_model import LogisticRegressionCV\n",
    "import numpy\n",
    "\n",
    "# Data Preparation, get training data, isolate into predictors and response columns\n",
    "cent_train = pd.read_csv('diabetes_prediction_dataset_train.csv')\n",
    "X_train = cent_train.loc[:, cent_train.columns != 'diabetes'].to_numpy()\n",
    "Y_train = cent_train.loc[:, cent_train.columns == 'diabetes'].to_numpy().ravel()\n",
    "\n",
    "# Confirm shape should be cent_train: (n, 16), X_train: (n, 15), and Y_train (n,) where n is ~90000\n",
    "print('Size:\\n\\tcent_train:\\t{}\\n\\tX_train:\\t{}\\n\\tY_train:\\t{}'.format(cent_train.shape, X_train.shape, Y_train.shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6abcd2fe-b7a7-43d3-853b-f0fceb8eeaf4",
   "metadata": {},
   "source": [
    "#### Model Definition and Training\n",
    "Here, the logistic regression model is defined. `LogisticRegressionCV` has a `cv` parameter which governs the number of folds in the cross-validation method. Because we are performing 10-fold cross-validation, we set `cv=10`. Finally, we tell select to use the Stochastic Average Gradient decent algorithm as per scikit's recommendations since it runs faster for larger datasets. Therefore, we set `solver='sag'`. SAG is also only compatible with the L2 penalty term so we need to set the `penalty` parameter to `penalty='l2'`. `random_state` is another parameter which should be set when using SAG. We set `random_state=0`. We also set the `max_iter` parameter to `10000` which halts learning when the model converges or when the maximum number of training iterations/epochs has been reached; whichever comes first. The `n_jobs` parameter is set to 5 to enable multithreaded training on 5 CPU cores. This speeds up model training by simultaneously training the model with 5 of the 10 folds. `verbose=1` simply allows us to view the training progress. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0254444d-26a3-4c01-b26d-6b2dddebe34a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model Definition\n",
    "cModel_LR = LogisticRegressionCV(cv=10, solver='sag', max_iter=10000, random_state=0, penalty='l2', n_jobs=5, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8feb338b-e11e-4c94-b098-4157ddbadd27",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=5)]: Using backend ThreadingBackend with 5 concurrent workers.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "convergence after 948 epochs took 70 seconds\n",
      "convergence after 947 epochs took 73 seconds\n",
      "convergence after 947 epochs took 73 seconds\n",
      "convergence after 947 epochs took 74 seconds\n",
      "convergence after 947 epochs took 75 seconds\n",
      "convergence after 939 epochs took 64 seconds\n",
      "convergence after 938 epochs took 67 seconds\n",
      "convergence after 938 epochs took 68 seconds\n",
      "convergence after 938 epochs took 71 seconds\n",
      "convergence after 941 epochs took 70 seconds\n",
      "convergence after 1060 epochs took 72 seconds\n",
      "convergence after 1068 epochs took 74 seconds\n",
      "convergence after 1066 epochs took 75 seconds\n",
      "convergence after 1056 epochs took 77 seconds\n",
      "convergence after 1067 epochs took 78 seconds\n",
      "convergence after 748 epochs took 48 seconds\n",
      "convergence after 751 epochs took 52 seconds\n",
      "convergence after 751 epochs took 52 seconds\n",
      "convergence after 747 epochs took 53 seconds\n",
      "convergence after 756 epochs took 53 seconds\n",
      "convergence after 1092 epochs took 72 seconds\n",
      "convergence after 1101 epochs took 75 seconds\n",
      "convergence after 1101 epochs took 76 seconds\n",
      "convergence after 1089 epochs took 79 seconds\n",
      "convergence after 1124 epochs took 80 seconds\n",
      "convergence after 482 epochs took 31 seconds\n",
      "convergence after 86 epochs took 7 seconds\n",
      "convergence after 12 epochs took 1 seconds\n",
      "convergence after 2 epochs took 0 seconds\n",
      "convergence after 2 epochs took 0 seconds\n",
      "convergence after 488 epochs took 36 seconds\n",
      "convergence after 488 epochs took 39 seconds\n",
      "convergence after 88 epochs took 7 seconds\n",
      "convergence after 12 epochs took 0 seconds\n",
      "convergence after 2 epochs took 0 seconds\n",
      "convergence after 2 epochs took 1 seconds\n",
      "convergence after 88 epochs took 7 seconds\n",
      "convergence after 12 epochs took 1 seconds\n",
      "convergence after 2 epochs took 0 seconds\n",
      "convergence after 2 epochs took 0 seconds\n",
      "convergence after 481 epochs took 39 seconds\n",
      "convergence after 509 epochs took 40 seconds\n",
      "convergence after 86 epochs took 6 seconds\n",
      "convergence after 12 epochs took 1 seconds\n",
      "convergence after 2 epochs took 0 seconds\n",
      "convergence after 2 epochs took 0 seconds\n",
      "convergence after 92 epochs took 7 seconds\n",
      "convergence after 13 epochs took 1 seconds\n",
      "convergence after 2 epochs took 0 seconds\n",
      "convergence after 2 epochs took 0 seconds\n",
      "convergence after 945 epochs took 63 seconds\n",
      "convergence after 949 epochs took 68 seconds\n",
      "convergence after 945 epochs took 69 seconds\n",
      "convergence after 945 epochs took 72 seconds\n",
      "convergence after 941 epochs took 71 seconds\n",
      "convergence after 938 epochs took 66 seconds\n",
      "convergence after 938 epochs took 67 seconds\n",
      "convergence after 940 epochs took 70 seconds\n",
      "convergence after 938 epochs took 73 seconds\n",
      "convergence after 940 epochs took 74 seconds\n",
      "convergence after 1064 epochs took 76 seconds\n",
      "convergence after 1063 epochs took 73 seconds\n",
      "convergence after 1063 epochs took 75 seconds\n",
      "convergence after 750 epochs took 53 seconds\n",
      "convergence after 1065 epochs took 78 seconds\n",
      "convergence after 1065 epochs took 77 seconds\n",
      "convergence after 755 epochs took 52 seconds\n",
      "convergence after 748 epochs took 52 seconds\n",
      "convergence after 750 epochs took 54 seconds\n",
      "convergence after 748 epochs took 53 seconds\n",
      "convergence after 1100 epochs took 72 seconds\n",
      "convergence after 1118 epochs took 80 seconds\n",
      "convergence after 487 epochs took 31 seconds\n",
      "convergence after 1093 epochs took 74 seconds\n",
      "convergence after 87 epochs took 6 seconds\n",
      "convergence after 13 epochs took 1 seconds\n",
      "convergence after 2 epochs took 0 seconds\n",
      "convergence after 2 epochs took 0 seconds\n",
      "convergence after 1093 epochs took 75 seconds\n",
      "convergence after 1099 epochs took 77 seconds\n",
      "convergence after 504 epochs took 31 seconds\n",
      "convergence after 483 epochs took 30 seconds\n",
      "convergence after 91 epochs took 6 seconds\n",
      "convergence after 13 epochs took 0 seconds\n",
      "convergence after 2 epochs took 1 seconds\n",
      "convergence after 2 epochs took 0 seconds\n",
      "convergence after 86 epochs took 4 seconds\n",
      "convergence after 13 epochs took 1 seconds\n",
      "convergence after 2 epochs took 0 seconds\n",
      "convergence after 2 epochs took 0 seconds\n",
      "convergence after 483 epochs took 24 seconds\n",
      "convergence after 488 epochs took 26 seconds\n",
      "convergence after 87 epochs took 9 seconds\n",
      "convergence after 12 epochs took 1 seconds\n",
      "convergence after 2 epochs took 0 seconds\n",
      "convergence after 2 epochs took 0 seconds\n",
      "convergence after 87 epochs took 10 seconds\n",
      "convergence after 12 epochs took 0 seconds\n",
      "convergence after 2 epochs took 0 seconds\n",
      "convergence after 2 epochs took 1 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=5)]: Done  10 out of  10 | elapsed: 13.2min finished\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-1 {color: black;background-color: white;}#sk-container-id-1 pre{padding: 0;}#sk-container-id-1 div.sk-toggleable {background-color: white;}#sk-container-id-1 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-1 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-1 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-1 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-1 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-1 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-1 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-1 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-1 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-1 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-1 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-1 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-1 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-1 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-1 div.sk-item {position: relative;z-index: 1;}#sk-container-id-1 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-1 div.sk-item::before, #sk-container-id-1 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-1 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-1 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-1 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-1 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-1 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-1 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-1 div.sk-label-container {text-align: center;}#sk-container-id-1 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-1 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-1\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>LogisticRegressionCV(cv=10, max_iter=10000, n_jobs=5, random_state=0,\n",
       "                     solver=&#x27;sag&#x27;, verbose=1)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-1\" type=\"checkbox\" checked><label for=\"sk-estimator-id-1\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">LogisticRegressionCV</label><div class=\"sk-toggleable__content\"><pre>LogisticRegressionCV(cv=10, max_iter=10000, n_jobs=5, random_state=0,\n",
       "                     solver=&#x27;sag&#x27;, verbose=1)</pre></div></div></div></div></div>"
      ],
      "text/plain": [
       "LogisticRegressionCV(cv=10, max_iter=10000, n_jobs=5, random_state=0,\n",
       "                     solver='sag', verbose=1)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Model Training\n",
    "cModel_LR.fit(X_train, Y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f76a80c2-f773-4770-9aaa-dc508f1ae279",
   "metadata": {},
   "source": [
    "#### Centralized Model Evaluation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "dff78939-7fbe-4e55-93e7-55a25b3a1433",
   "metadata": {},
   "outputs": [],
   "source": [
    "cent_test = pd.read_csv('diabetes_prediction_dataset_test.csv')\n",
    "X_test = cent_train.loc[:, cent_train.columns != 'diabetes'].to_numpy()\n",
    "Y_test = cent_train.loc[:, cent_train.columns == 'diabetes'].to_numpy().ravel()\n",
    "y_pred = cModel_LR.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "391b1477-3994-469e-ab2c-453ae0051d83",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.97      0.99      0.98     82374\n",
      "           1       0.87      0.62      0.73      7625\n",
      "\n",
      "    accuracy                           0.96     89999\n",
      "   macro avg       0.92      0.81      0.85     89999\n",
      "weighted avg       0.96      0.96      0.96     89999\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn import metrics\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "print(classification_report(Y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "350f5483-9a4a-45a4-9f69-e994a0227c2c",
   "metadata": {},
   "source": [
    "### DELETE ME BUT GOOD REFERENCES \n",
    "https://www.datacamp.com/tutorial/understanding-logistic-regression-python\n",
    "https://www.section.io/engineering-education/how-to-implement-k-fold-cross-validation/\n",
    "https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.to_numpy.html\n",
    "https://www.google.com/search?client=firefox-b-d&q=the+max_iter+was+reached+which+means\n",
    "https://www.google.com/search?client=firefox-b-d&q=can+sklearn+use+gpu\n",
    "https://www.google.com/search?q=scikit+learn+replace+model+coefficients&client=firefox-b-d&sxsrf=APwXEdeNYOhw-LS79iqhPKrhPg5uXq_VXg%3A1682610796889&ei=bJpKZM7uNbOcptQP5eCW6AE&oq=scikit+learn+replace+model+coe&gs_lcp=Cgxnd3Mtd2l6LXNlcnAQAxgAMgUIIRCgAToKCAAQRxDWBBCwAzoECCMQJzoHCAAQigUQQzoICAAQigUQkQI6BQgAEIAEOgoIABCABBAUEIcCOgYIABAWEB46CAgAEBYQHhAPOggIABAWEB4QCjoICCEQFhAeEB06BAghEBVKBAhBGABQzRxYmHJgiIABaAFwAXgAgAHjAYgBiBCSAQY3LjEwLjGYAQCgAQHIAQjAAQE&sclient=gws-wiz-serp\n",
    "https://stackoverflow.com/questions/24438779/creating-a-sklearn-linear-model-logisticregression-instance-from-existing-coeffi\n",
    "https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LogisticRegressionCV.html#sklearn.linear_model.LogisticRegressionCV\n",
    "https://www.google.com/search?q=sklearn+logistic+regression&client=firefox-b-d&sxsrf=APwXEde385EBa7GBKCN58NQH7JRKZx_gcQ%3A1682602212713&ei=5HhKZI-dK5upptQP7ZiZ-Ac&oq=sklearn+&gs_lcp=Cgxnd3Mtd2l6LXNlcnAQAxgBMgQIIxAnMgcIABCKBRBDMgcIABCKBRBDMgcIABCKBRBDMgcIABCKBRBDMgcIABCKBRBDMgcIABCKBRBDMggIABCKBRCRAjIHCAAQigUQQzIICAAQigUQkQI6CggAEEcQ1gQQsAM6CggAEIoFELEDEENKBAhBGABQ4BJYzh1gqS5oAnABeACAAXaIAfAFkgEDNC40mAEAoAEByAEIwAEB&sclient=gws-wiz-serp#ip=1"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
